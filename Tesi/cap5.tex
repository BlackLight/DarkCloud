\chapter{Risultati sperimentali}
Al termine della fase di programmazione il programma Darkcloud risulta pienamente funzionante. Solitamente durante lo sviluppo di un software per verificarne il funzionamento e testare la funzionalità dei comandi si fanno continui test sulle macchine degli sviluppatori e su altre architetture. Anche nel nostro caso abbiamo svolto diversi test nella fase di development. Giunti ad avere un software funzionante un modo per testarne la qualità è effettuare dei test sulle prestazioni. I test di prestazioni sui software possono essere di diversi tipi, noi abbiamo effettuati alcuni test sulla velocità e sul carico di lavoro.
Sapere se la velocità di esecuzione di un comando è compatibile con i tempi di utilizzo normali è importante, in quanto se andassero a rallentare quello che è il lavoro dell'utilizzatore ultimo sarebbe controproducente. Oggi se un computer è troppo lento o un programma impiega troppo tempo, e come nel nostro caso l'utilizzatore è un professionista che ha bisogno di efficienza nel lavoro, questo ultimo non ci penserà due volte a cambiare prodotto. Un altro parametro importante è il carico di lavoro, cioè la fatica che il computer fa per per eseguire un certo comando. E' vero che oggi i computer sono molto potenti, ma l'aumento della portabilità dei dispositivi porterà anche a riduzioni della potenza di calcolo. Per quanto riguarda la compatibilità su diverse piattaforme e sistemi operativi il nostro programma è molto avvantaggiato, in quanto è scritto in java. Java è un linguaggio indipendente dalla piattaforma, infatti i programmi scritti in linguaggio Java sono destinati all'esecuzione sulla piattaforma Java, ovvero saranno lanciati su una Java Virtual Machine e, a tempo di esecuzione, avranno accesso alle API della libreria standard. Ciò fornisce un livello di astrazione che permette alle applicazioni di essere interamente indipendenti dal sistema su cui esse saranno eseguite.
\section{Ambienti di test}
I nostri test sono stati effettuati principalmente su due sistemi. Il personal computer del tesista,che chiameremo calcolatore A, e il server universitario Capoeira.
Le configurazioni hardware dei due sistemi sono molto differenti. Le componenti che faranno la differenza nelle prestazioni sono la cpu, la memoria e il disco fisso.
Il calcolatore A ha una cpu Intel i5-750, un modulo di memoria ram da 4gb e disco rigido ssd. Il server Capoeira ha 2 cpu Intel E5540, 16gb di memoria ram e dischi scsi a 15000 rpm. La differenza che più sentiremo nei test riguarda i processori, infatti il server ha una capacità di calcolo superiore di quasi due volte a quella del calcolatore A. Il processore intel i5-750 infatti ha quattro core mentre ogni singolo E5540 ha quattro core, moltiplicato per due processori fa 8 core. Il calcolatore A ha però i core con frequenza fissa a 3.6 ghz mentre quelli di capoeira hanno una frequenza che varia in base alla richiesta.
Il sistema operativo sul calcolatore A è Windows 7 64bit sul quale tramite VirtualBox è stato usato in macchina virtuale il sistema operativo Debian 4.3.5 basato sul kernel linux 2.6.32. Si è optato per questa soluzione per semplificare l'utilizzo degli strumenti di sviluppo e test. Il server Capoeira invece ha installato Debian 4.5.3 basato sul kernel linux 3.0.0. 
\section{Risultati sperimentali}
Per misurare i tempi impiegati dai comandi dall'avvio al termine dell'esecuzione si è impiegato il programma linux TIME. Questo comando anteposto ad un altro restituisce alla fine dell'esecuzione il tempo totale impiegato dal processo e la percentuale di cpu utilizzata per eseguirlo. 
I comandi utilizzabili al momento sono il ping, il put, il get e lo share. Il ping è un comando usato solo per verificare la disponibilità in rete di una macchina, quindi su di esso non sono stati fatti test prestazionali. Sono invece stati testati il put, il get e lo share. Un esempio di sintassi del test è il seguente
\begin{verbatim}time python client.py -r PUT -h 127.0.0.1 -p 18100 -f prova.txt 
-F prova.txt\end{verbatim}\\
I test sono stati fatti tutti creando i nodi in locale quindi non tengono conto di eventuali ritardi dovuti a latenze della rete. Il fattore delle latenze di rete non è  
quantificabile a priori, senza conoscere i casi di uso specifici, quindi per il momento è stato ignorato. Facendo test in locale possiamo capire come reagisce il software quando le istanze aumentano e il carico dei dati si incrementa. Possiamo dire che l'obbiettivo dei nostri test è capire se il software gestisce bene l'aumento dei nodi, che comporta a seconda del comando un aumento della complessità dell'architettura e ci permette di valutare la scalabilità del programma. 

Per testare il put abbiamo creato in locale una architettura che comprendeva un nodo client e cinque nodi server. Si è iniziato con l'invio di un file ad un solo server, poi verso due server incrementando il numero fino all'invio a cinque server. Per ogni caso la misurazione è stata eseguita dieci volte e al termine raccolti i risultati abbiamo fatto la media. Il file inviato era sempre della stessa dimensione, cioè 1KB, in modo da uniformare i risultati. In questo test l'aumento del numero dei server comporta un aumento del codice che il client deve eseguire, osservare i tempi e i carichi ci dirà se il software è in grado di incrementare le proprie prestazioni o se ci sono colli di bottiglia che rallentano.

Ecco di seguito i test sul put.
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°server & tempo di esecuzione & uso cpu\\
\hline
1 & 1,2 sec & 3\%\\
\hline
2 & 1,9 sec & 2\%\\
\hline
3 & 2,6 sec & 1\%\\
\hline
4 & 3,3 sec & 1\%\\
\hline
5 & 4 sec & 1\%\\
\hline
\end{tabular}
\end{center}
\caption{Test su Capoeira del comando PUT}
\label{tab:table}
\end{table}
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°server & tempo di esecuzione & uso cpu\\
\hline
1 & 1,8 sec & 1\%\\
\hline
2 & 2,8 sec & 1\%\\
\hline
3 & 4,1 sec & 1\%\\
\hline
4 & 5,7 sec & 1\%\\
\hline
5 & 6,1 sec & 1\%\\
\hline
\end{tabular}
\end{center}
\caption{Test sul calcolatore A del comando PUT}
\label{tab:table}
\end{table}

I test eseguiti sul comando get vanno invece a testare il tempo di risposta e il carico di lavoro a cui si sottopone la cpu quando si devono recuperare dai server le informazioni per ricomporre il file. Anche in questo caso abbiamo utilizzato file di dimensione uguale, tutti di 1KB. Il programma utilizzato è stato time con una sintassi simile alla seguente:
\begin{verbatim}time python client.py -r GET -h 127.0.0.1 -p 18100 -F prova.txt\end{verbatim}\\
Come nel get per testare la scalabilità del software nell'aumento della difficoltà computazionale abbiamo incrementato gradualmente i nodi dai quali andavamo a prelevare l'informazione. In questo modo il programma deve eseguire più comandi. Prima abbiamo recuperato dal nostro client un file contenuto su un nodo, poi su due nodi incrementando fino a cinque nodi. Per ognuna di queste architetture si sono fatti dieci rilevamenti e i valori in tabella ne sono le medie. Si riportano di seguito le tabelle dei risultati dei test del get.
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°server & tempo di esecuzione & uso cpu\\
\hline
1 & 1,4 sec & 3\%\\
\hline
2 & 2,1 sec & 2\%\\
\hline
3 & 2,3 sec & 2\%\\
\hline
4 & 2,8 sec & 1\%\\
\hline
5 & 3,6 sec & 1\%\\
\hline
\end{tabular}
\end{center}
\caption{Test su Capoeira del comando GET}
\label{tab:table}
\end{table}
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°server & tempo di esecuzione & uso cpu\\
\hline
1 & 2,1 sec & 1\%\\
\hline
2 & 3,0 sec & 1\%\\
\hline
3 & 3,6 sec & 1\%\\
\hline
4 & 4,2 sec & 1\%\\
\hline
5 & 6,5 sec & 2\%\\
\hline
\end{tabular}
\end{center}
\caption{Test sul calcolatore A del comando GET}
\label{tab:table}
\end{table}
L'ultimo comando da analizzare è lo share, a differenza del put e del get questo comando non fa interagire nodi client con nodi server, ma solo nodi client tra di loro. Quindi nei test di questo comando non sarebbe utile ai fini dell'analisi della scalabilità l'aumento del numero dei server, ma piuttosto l'aumento del numero dei client con cui un nodo client voglia condividere un informazione. Per questo motivo in questi test abbiamo utilizzato il comando share con un numero sempre maggiore di client come obbiettivo, prima con un solo client poi con due crescendo fino a cinque client. Per fare questo abbiamo incontrato il problema che lo share accetta solo un altro client come destinatario dello share. Per ovviare questo problema abbiamo realizzato uno script shell all'interno del quale abbiamo scritto tanti comandi share concanetati dall'operatore \& che nella console permette di eseguire i comandi in background. In questo modo abbiamo inviato contemporaneamente le richieste di share e le abbiamo fatte processare tutte contemporaneamente. Usando poi il programma time seguito dallo script abbiamo ottenuto il tempo totale dell'esecuzione dei comandi share. La sintassi del file script.sh per lo share delle informazioni di un file da un nodo verso altri due per esempio sarebbe
\begin{verbatim}python client -r SHARE -h 127.0.0.1 -p 18100 -F prova.txt -H 127.0.0.1 
-P 18101 & python client -r SHARE -h 127.0.0.1 -p 18100 -F prova.txt 
-H 127.0.0.1 -P 18101\end{verbatim}\\
Mentre invece la sintassi del comando time con lo script è
\begin{verbatim}time ./script.sh\end{verbatim}\\
Per ogni architettura di prova sono stati eseguiti dieci test e il valore riportato in tabella rappresenta la media. Il file condiviso era situato su un solo server in modo da poter focalizzare i test sulla scalabilità dello share e non della dimensione dei dati salvati, inoltre tale file era delle dimensioni di 1KB. Si riportano di seguito le tabelle dei test dello share.
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°client & tempo di esecuzione & uso cpu\\
\hline
1 & 1,4 sec & 3\%\\
\hline
2 & 1,5 sec & 3\%\\
\hline
3 & 1,6 sec & 7\%\\
\hline
4 & 1,7 sec & 8\%\\
\hline
5 & 2,2 sec & 8\%\\
\hline
\end{tabular}
\end{center}
\caption{Test su Capoeira del comando SHARE}
\label{tab:table}
\end{table}
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
n°client & tempo di esecuzione & uso cpu\\
\hline
1 & 1,9 sec & 1\%\\
\hline
2 & 2,5 sec & 1\%\\
\hline
3 & 3,6 sec & 1\%\\
\hline
4 & 4,2 sec & 1\%\\
\hline
5 & 7 sec & 1\%\\
\hline
\end{tabular}
\end{center}
\caption{Test sul calcolatore A del comando SHARE}
\label{tab:table}
\end{table}\\
Ad una prima analisi i risultati potrebbero sembrare strani visto che su Capoeira l'utilizzo della cpu aumenta mentre sul calcolatore A rimane basso. Questi valori sono dovuti al tipo di processori usati. Infatti il calcolatore A ha un processore da desktop con frequenza molto alta, risparmi energetici disattivati e alto consumo quindi ha il processore sempre ad un regime di lavoro alto. I processori dei server sono più dinamici dovendo pensare anche al consumo dato che sono in numero molto maggiore. La potenza data dall'utilizzo di 8 core nel server però si nota dai tempi molto più bassi. I risultati sono positivi anche per quanto riguarda la scalabilità in quanto l'aumento dei tempi è lineare e questo indica che il software gestisce bene l'incremento di complessità computazionale. 