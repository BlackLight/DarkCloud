\chapter{Stato dell'arte}
In questo capitolo vengono esposti i protocolli che oggi rappresentano lo standard de facto nella condivisione dei file. Anche se molti sono conosciuti come programmi usati per scambio di materiale multimediale i protocolli che vi sono alla base sono utilizzati da tutti i programmi presenti sul mercato usati dai professionisti per condividere file. 
\section{Napster}
\begin{figure}
\begin{center}
\includegraphics[width=15.3cm]{img/napster.png}
\end{center}
\caption{Architettura di Napster}
\label{Architettura Napster}
\end{figure}
Napster\cite{napster} è stato il primo software di p2p ad avere una diffusione di un certo livello. Nacque nel 1999 da un progetto di Shawn Fanning.
Non si tratta di un sistema p2p puro in quanto manteneva una lista di tutti gli utenti connessi e dei loro file su dei server centrali raggiungibili da tutti.
Quindi se un nodo voleva trovare un file mandava la richiesta al server centrale che rispondeva con la lista dei nodi che lo avevano e il richiedente poteva fare partire il download da tali nodi. Quando un utente decideva di condividere un file mandava i dettagli al server centrale che li salvava.
Quindi le condivisioni vere e proprie avvenivano tra gli utenti come un programma di istant messaging avanzato.
I vantaggi di questo tipo in questo tipo di struttura risiedono nella velocità di reperibilità delle informazioni in quanto l'elenco dei file era disponibile a tutti direttamente. Altro punto a suo favore era la velocità di condivisione, in quanto i nodi si scambiavano i file creando una connessione diretta tra di loro, e il file era in chiaro e completo. Quelle che sono le sue caratteristiche positive in prima analisi si rivelano i suoi maggiori punti deboli se visti dalla prospettiva di utenti male intenzionati. Per esempio se il server centrale cade, tutta la rete smette di funzionare. Oppure se un ente vuole proibire la diffusione di certo materiale gli basterà confiscare il server per sapere chi ne era il detentore e con chi lo ha condiviso. La connessione diretta tra gli utenti porta inoltre tutta una serie di punti deboli come la facile trasmissione di infezioni informatiche, l'impossibilità di reperire un dato se il suo condivisore è offline e la mancanza di certificati di autenticità permette ad utenti di modificare contenuti e re immetterli in rete.
\section{Gnutella}
\begin{figure}
\begin{center}
\includegraphics[width=15.3cm]{img/gnutella.png}
\end{center}
\caption{Architettura di Gnutella}
\label{Architettura Gnutella}
\end{figure}
Gnutella\cite{gnutella} a differenza di Napster è un software peer to peer puro. Cioè non ha alcun server che centralizza alcun tipo di informazione.
I computer degli utenti sono gli unici nodi che compongono la rete. Quando un certo numero di utenti installa il software sulla propria macchina automaticamente Gnutella riconosce i nodi vicini e in questo modo la rete si amplia. Le ricerche avvengono secondo la tecnica del flodding, ovvero quando un nodo deve reperire un file manda una richiesta ai nodi vicini, se i nodi vicini non hanno il file richiesto inoltrano a loro volta a tutti nodi che conoscono la stessa domanda tranne al nodo che gli ha mandato la richiesta originale. Per evitare il collasso della rete le richieste avevano un numero massimo di 'salti' tra nodi, al termine del quale venivano cancellate. Anche i trasferimenti di file vengono sostenuti dai nodi intermedi tra richiedente e possessore. 
Questo protocollo ha il vantaggio di essere potenzialmente inarrestabile, fortemente stabile e molto dinamico permettendo a nodi di entrare e uscire senza modificare le prestazioni della rete. Non teme quindi attacchi che ne arrestino il funzionamento. 
La tecnica di ricerca risulta molto rudimentale, non garantisce di trovare un file e i tempi per trovarlo comunque possono essere molto lunghi.
\section{Kademlia}
\begin{figure}
\begin{center}
\includegraphics[width=15.3cm]{img/kademlia.png}
\end{center}
\caption{Indici dei file in Kademlia}
\label{Indici dei file in Kademlia}
\end{figure}
E' un protocollo di rete peer to peer decentralizzato ideato da Petar Maymounkov e David Mazières della New York University\cite{kad}.
E' il primo protocollo analizzato che sfrutta la tecnologia delle DHT ovvero Distributed Hash Tables. La tecnica delle DHT nasce dalla collaborazione di alcune comunità di file sharing e viene subito adottata in diversi software\cite{kademlia}.
Il metodo di mantenimento dell'indice dei file presenti sulla rete, DHT appunto, è il suo punto forte. Fino a quel momento il reperimento dei file era totalmente centralizzato mantenendo le liste in uno o più server oppure totalmente distribuito il che rendeva necessario contattare moltissimi nodi per trovare i file.
Per capire come funzionino invece le DHT si immagini una lista di tutti i file presenti in una rete p2p, ora si frammenti questa lista in tante parti e le si distribuisca ai nodi che la compongono. Questi pezzi di lista sono replicati per permetterne il reperimento anche quando alcuni nodi sono offline.
Ogni nodo oltre ad avere il suo pezzetto di lista sa anche in che direzione la lista si propaga, in questo modo se un nodo a lui vicino gli chiede dove si trova un file, e lui non lo sa, potrà indicargli in che direzione continuare a cercarlo. Una volta che il nodo richiedente trova nella lista il file che vuole, legge gli ip dei nodi che hanno il file e procede al recupero del file. La pubblicazione 'A distributed hash table'\cite{dht} di Frank Dabek, professore in Computer Science del MIT tratta in modo approfondito ed accurato le DHT.
Nelle DHT il reperimento dei file è molto più veloce di quanto non lo sarebbe in una rete completamente decentralizzata dove è necessario fare rimbalzare la richiesta verso moltissimi nodi.
E' anche più sicuro del metodo centralizzato, non solo dal punto di vista della privacy ma anche di stabilità della rete stessa e di reperibilità dei file.
Dato che non esiste nessun tipo di centralizzazione e i nodi sono responsabili dei file che caricano però, se tutti i nodi che conoscono un file si spengono l'intera rete non riuscirà più a raggiungere quel file.
\section{Le darknet e Freenet}
\begin{figure}
\begin{center}
\includegraphics[width=15.3cm]{img/freenet.png}
\end{center}
\caption{Esempio protocollo Freenet}
\label{Protocollo Freenet}
\end{figure}

Le darknet sono reti per condividere file nate per tutelare la privacy e combattere la censura.
Infatti in alcuni paesi lo stato proibisce di condividere file che dicano cose che non sono da esso approvate, e per permettere la libera
diffusione di informazioni alcuni Informatici hanno pensato ad un modello di rete altamente dinamico e anonimo.
Il termine darknet è stato coniato prima della nascita di internet nel 1970, quando l'unica rete riconosciuta era ARPANET e identificava
una rete che fosse isolata da ARPANET per motivi di sicurezza. Questo non vuol dire che ne fosse separata, infatti i terminali che appartenevano a
quelle darknet potevano ricevere ed inviare dati attraverso ARPANET, ma non risultavano registrati da nessuna parte e non potevano essere rintracciati
in quanto non rispondevano ai segnali che chiedevano di identificarsi.
Con il tempo e l'avvento di internet si iniziò ad usare il termine darknet per indicare reti private che passavano per internet usate da alcuni utenti per scambiarsi file, ma sempre e soltanto in piccoli gruppi che si conoscevano.
Questo termine prima sconosciuto ai profani è stato dato in pasto ai media a seguito della pubblicazione del documento 'The Darknet and the Future of Content Distribution' \cite{thedarknet} scritto da quattro ricercatori della Microsoft.
Questo documento elencava tre punti essenziali che permettevano di identificare una rete come darknet: 
\begin{enumerate}
	\item ogni oggetto distribuito deve essere condiviso sulle macchine di una parte degli utenti, in una forma che ne consenta la copia
	\item tutti gli utenti possono ottenere una copia dei file se ne hanno interesse
	\item tutti gli utenti devono essere connessi tramite banda larga
\end{enumerate}
Questo documento fu condiviso su larga scala e interpretato dai tutori del diritto di autore come il principale impedimento alla diffusione 
di materiale protetto in forma elettronica.
Quindi si può spesso sentire parlare di programmi di condivisione come furono Napster, Gnutella, Kademlia e molti altri sotto la definizione di darknet da parte dei media, ma in realtà non è così. Questi sono programmi di p2p, peer to peer, differenti dalle darknet a causa della accessibilità delle reti a chiunque installi semplicemente il software.
La tipologia di rete che si avvicinava di più al concetto di darknet erano le reti f2f, friend to friend.
Ovvero reti dove si era collegati solo a nodi fidati e conosciuti, tramite porte e protocolli non standard. 
Il concetto di darknet però è stato ulteriormente rivoluzionato attorno al 2005 con la nascita di reti p2p ANONIME.
Reti nelle quali è possibile condividere file senza il rischio di essere rintracciati.
Proprio per questa caratteristica le reti darknet sono molto criticate, perché oltre che per preservare la libertà di informazione possono essere usate per condividere materiale illegale e dannoso.

Al momento molti ricercatori stanno studiando protocolli alternativi, che integrano complicati metodi di routing per garantire l'anonimato e al tempo stesso mantenere performance adeguate. Si può menzionare il protocollo R5N\cite{r5n} sviluppato al TUM, Università tecnica di Monaco, che utilizza le DHT e metodi di randomizzazione delle comunicazioni tra nodi, o ancora un programma fatto all'università svedese di Uppsala che si chiama OneSwarm\cite{bitdark} nel quale si è pensato di realizzare una rete bitTorrent usando dei nodi di cloud computing.

Il programma studiato nella categoria darknet è Freenet\cite{freenet}. Questa scelta è stata dettata dalla grande comunità che gli sta alle spalle, dal fatto che sia disponibile il codice sorgente e dalle caratteristiche che più si avvicinavano al modello Darkcloud. Molto utile in questo senso è stato lo studio della pubblicazione 'Beyond Simulation: Large-Scale Distribuited Emulation of P2P Protocols'\cite{beyond}, prodotta da due ricercatori tedeschi, nella quale si evidenza la forte differenza di scalabilità a seconda del tipo di topologia di una rete. Freenet è una rete decentralizzata dove non ci sono server e tutte le informazioni sono distribuite sui nodi. E' una rete realizzata puntando ad un alto livello di sicurezza ed anonimato. 
I file sono salvati sui nodi in modo crittografato e replicati su diversi nodi. Questo fa si che chi ha installato il programma non conosca cosa stia condividendo. Non esiste una struttura gerarchica tra i nodi, i collegamenti si basano sulla vicinanza, ogni nodo conosce solo quelli vicino a lui. Quindi quando vengono aggiunti nuovo nodi non si può prevedere a chi saranno collegati. Ogni file è contraddistinto da un id univoco, e il meccanismo per gestire l'indicizzazione dei file è simile alle DHT però invece che basarsi su una lista dei file che un nodo possiede si basa sulla velocità con la quale un nodo può avere un file. Dopo che un nodo ha trovato il file la richiesta passa di nodo in nodo, fino a raggiungere il nodo con il file. In seguito il nodo possessore del file lo invia, e il file viene trasmesso tra tutti i nodi intermedi. Quando vengono caricati nella rete i file non rimangono nel nodo che li ha condivisi, ma vengono replicati in modo da essere disponibili ad una certa velocità per tutti i nodi della rete. Dato che il modo per indicizzare i file si basa sulla velocità alla quale un file è disponibile questo vuol dire creare una ridondanza dei file su tutta la rete in modo da essere disponibili più velocemente per tutti! 
Oltre al funzionamento sulla topografia sopra descritta, cioè sfruttando tutti i pc con installato Freenet presenti sulla rete internet, il software offre una modalità di funzionamento friend to friend. In questo setting è necessario specificare una lista di nodi manualmente questi saranno gli unici con cui andremo a condividere dei file.
E' una opportunità in più tramite la quale si condivide solo con i propri amici. Può essere utile per esempio per alcuni gruppi di utenti che condividono file riservati e non vogliono renderli disponibili a tutta la rete Freenet.
Ogni caratteristica di questo programma è atta ad aumentarne l'anonimato e si può constatare che funziona molto bene.
Tuttavia queste scelte sono state fatte essendo consapevoli che i file condivisi sarebbero stati documenti di piccole dimensioni. Questo rende lenta e difficile la gestione di file di dimensioni maggiori.
\section{Cloud computing}
\begin{figure}
\begin{center}
\includegraphics[width=15.3cm]{img/cloud.png}
\end{center}
\caption{Struttura Cloud Computing}
\label{Struttura Cloud Computing}
\end{figure}
Di cloud computing si sente parlare già dal 1960, quando lo scienziato informatico John McCarthy ipotizza un futuro nel quale la capacità di calcolo sarebbe stata organizzata come un servizio di pubblica utilità. Naturalmente i primi passi furono ben distanti, infatti le grandi aziende che con gli anni iniziavano ad usare data center avevano delle macchine che erano dedicate a sopperire solo alle richieste di un cliente. Gli studiosi si accorsero che per la maggior parte del tempo però le macchine lavoravano al minimo, registrando picchi di calcolo solo in certi momenti della giornata. In questo modo vi era un enorme spreco di energia elettrica e di risorse. La prima multinazionale che notati questi sprechi si mise al lavoro per trovare una soluzione fu la americana Amazon. Infatti nel 2006 presentò Amazon Web Service\cite{amazon2} un insieme di servizi web. All'interno dei servizi proposti ve ne erano diversi di cloud computing, per esempio servizi che offrivano spazio online dinamico e veloce con costi in base al consumo, servizi che offrivano sistemi operativi virtuali utilizzabili online con costi basati sul tempo di utilizzo e server per applicazioni web nel quale il costo variava in base alle ore che il servizio era operante. Si può dire che è stata proprio questa la nascita del vero e proprio cloud computing ovvero servizi web che offrono spazio di archiviazione, capacità computazionale, sistemi operativi, hardware e banda in base alle esigenze in modo dinamico. Come la rete di distribuzione idrica o del servizio elettrico, dove si paga quello che si consuma. 

Negli ultimi tempi si è vista una esplosione di questo tipo di servizi, ormai quasi tutti i provider hanno a listino un servizio di cloud. Viste le potenzialità molte aziende del calibro di Apple e Microsoft stanno utilizzando questa tecnologia per dare un servizio importante ai propri utenti come il centralizzare i dati personali tra i tanti terminali che oggi si hanno, notebook, desktop, smartphone, tablet. E' facile prevedere che il cloud cambierà l'informatica da come la si conosce adesso.

Il cloud computing è una risorsa molto preziosa visti i moderni strumenti che mette a disposizione, non solo perché offre una grande potenza di calcolo o di spazio di archiviazione con un basso costo vista la dinamicità, ma anche perché i moderni strumenti usati dal cloud per virtualizzare permettono di avere un qualsiasi modello di hardware subito e di fare le modifiche necessarie a piacimento.
Dato che la rete Darkcloud deve rendere i dati sempre disponibili, non è possibile scegliere una struttura di rete che mantenga il dato sui client, ciò porterebbe molti rischi. Invece la tecnologia del cloud computing permette di avere una serie di macchine sempre online e con uno spazio di archiviazione e una banda passante di capacità che va ben oltre le necessità del caso studiato. 

I servizi che vengono offerti nel campo del cloud computing si possono dividere in tre tipologie. I servizi SaaS ovvero Sofware as a Service, PaaS Platform as a Service o IaaS Infrastructure as a Service. Il servizio IaaS permette l'utilizzo di risorse hardware in remoto, servizio simile al Grid Computing ma con una caratteristica imprescindibile: le risorse vengono elargite in base all'utilizzo effettivo. Tramite un servizio IaaS per esempio si possono avere diversi sistemi operativi attivi su server remoti o ancora servizi di storage online nei quali viene dato spazio di archiviazione. I servizi PaaS permettono di eseguire in remoto una piattaforma software formata da diversi programmi. Infine i servizi SaaS consentono l'utilizzo in remoto di singoli programmi. I programmi utilizzabili su SaaS però devono essere realizzati con ottimizzazioni per il servizio infatti ogni provider di servizi cloud ha una propria serie di librerie, che contengono i comandi ottimizzati per le loro infrastrutture. Questo offre il vantaggio di avere software molto veloce ed efficiente, essendo ottimizzato, d'altra parte però obbliga a sviluppare software ad hoc per ogni diverso servizio di cloud computing. Per esempio le API del servizio cloud di Google sono diverse da quelle di Amazon che sono ancora diverse di Microsoft. E' importante realizzare Darkcloud in modo che sia facile in un secondo momento adattarlo a diversi servizi di cloud, e questo è possibile utilizzando l'incapsulamento e la modularità nel codice in modo da renderlo facilmente riutilizzabile.

Lo stato attuale dell'utilizzo del cloud computing è molto avanzato, infatti la quasi totalità del siti web ora sono ospitati sul web sono su server che sfruttano tale tecnologia. Oltre a questo molti software che hanno una parte web sono ospitati su questi servizi. Molti ancora riservano dubbi a riguardo della sicurezza in questi campi, ma secondo l'opinione di diversi ricercatori\cite{amazon}\cite{cloudsec} il cloud offrirà possibilità che favoriranno molto la sicurezza, la differenza sarà nelle implementazioni. 